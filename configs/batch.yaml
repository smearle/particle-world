{
    # The names of the experiments. Add a range of integers to allow for multiple trials of experiments with the same 
    # hyperparameters.
    "exp_names": [
        # "4",
        # "4_gpu",
        # "5",
        "5_gpu",
        # "debug",
        "test_regret",
    ],

    # The name of the class corresponding to the environment we wich to generate/play.
    "env_classes": [
        "ParticleMazeEnv",  # A maze navigation task with 1 spawn and 1 goal.
        # "TouchStone",  # A WIP Minecraft navigation task.
    ],

    "generator_classes": [
        "TileFlipIndividual",
        # "NCA",
    ],

    # List of [n_gen_itr, n_play_itr] hyperparameters. These indicate the number of steps of generator-evolution and 
    # player-training to take, respectively, before switching to a phase of the opposite type. If either is -1, then
    # we iterate until generator/player convergence (or "staleness") is reached. 0 is an invalid value.
# NOTE: currently ignoring these intervals and doing as much world-evolution as possible in parallal to each player 
# training step when using RL to train players.
    "ngen_nplay": [
        [1, 1],
        # [3, 3],
        # [-1, 1],
    ],

    # List of [n_policies: int, quality_diversity: bool, objective: str] hyperparameters. If quality_diversity is True, 
    # objective must be None. ``contrastive`` requires > 1 policy. QD is currently only compatible with 3 (2 "diversity
    # measure" policies, 1 "protagonist" who provides the objective).
    "npol_qd_objectives_measures": [
        # [1, true, "min_solvable", ["emptiness", "symmetry"]],  # ...emptiness/symmetry of map as measures
        # [3, true, "contrastive", null],
        # [2, true, "contrastive", ["emptiness", "symmetry"]],
        # [3, true, "contrastive", ["emptiness", "symmetry"]],
        # [3, true, "regret", null],
        # [2, true, "regret", null],
        # [1, true, "regret", ["emptiness", "symmetry"]],

        [3, true, "min_solvable", null],  # QD using min_solvability of player 1 as objective, and rewards of players 2 and 3 as measures
        [1, false, "regret", null],
        # [2, false, "paired", null],
        # [2, false, "contrastive", null],
        # [3, false, "contrastive", null],
        # [1, false, "min_solvable", null],
        # [1, false, null, null]  # this trains on a fixed set of worlds
    ],
    
    # List of pairs of [fully_observable: bool, field_of_view: int, model_name: str] hyperparameters. If the agent 
    # receives full observations, then field_of_view is ignored (i.e. it is set to the width of the level, if we are 
    # also translating the observation (haven't implemented translation=False case yet)). The ``field_of_view`` is the 
    # number of cells the agent can see in each direction (excluding its own position).
# FIXME: currently just a placeholder when evolving players.
    "fullobs_fov_models_rotated": [
        # [true, -1, "feedforward", false],
        # [false, 2, "rnn", false],
        [false, 2, "rnn", true],
        # [true, -1, "feedforward", false],
    ]
}